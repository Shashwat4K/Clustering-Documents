{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: times new roman;font-weight: bold;font-size: 40px;\">Clustering Documents</h1>\n",
    "\n",
    "<p style=\"font-family: times new roman;font-size: 18px;\"> Created by: Shashwat Kadam </p>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: times new roman;font-size: 24px; font-weight:bold\">* Dataset description:</p>\n",
    "<p style=\"font-family: times new roman;font-size: 18px;\">For this mini-project, I am working on a public dataset created in University of California, Irvine (UCI). This is a dataset made especially for studying clustering analysis. The dataset is Bag of Words dataset, in which a vocabulary is present and the counts of the words (term-frequency) in different documents is provided. Following is a more detailed description:</p>\n",
    "\n",
    "<p style=\"font-family: times new roman;font-size: 18px;\"> The data for this assignment is a bag of words made on collection of KOS Blog entries. There are two different files provided: </p>\n",
    "<!--<p style=\"font-family: times new roman;font-size: 18px;color: red; font-weight:bold\">TODO: CHANGE THE DATASET FROM ENRON TO KOS / NIPS </p>-->\n",
    "<ul>\n",
    "    <li style=\"font-family: times new roman;font-size: 18px;\"> <b>docword.kos.txt</b> </li>\n",
    "    <li style=\"font-family: times new roman;font-size: 18px;\"> <b>vocab.kos.txt</b></li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: times new roman;font-size: 18px; font-weight: bold;\"> docword.kos.txt </p>\n",
    "<p style=\"font-family: times new roman;font-size: 18px;\"> For each text collection, D is the number of documents, W is the\n",
    "number of words in the vocabulary, and N is the total number of words\n",
    "in the collection (below, NNZ is the number of nonzero counts in the\n",
    "bag-of-words). After tokenization and removal of stopwords, the\n",
    "vocabulary of unique words was truncated by only keeping words that\n",
    "occurred more than ten times. </p>\n",
    "\n",
    "<p style=\"font-family: times new roman;font-size: 18px;\"> The file contains metadata of the collection, i.e. values of 'D', 'W' and 'N' as defined above (first 3 lines of the file).This file also contains tuples (docID, wordID, count) which tells that a word having ID <i>wordID</i> is present in document <i>docID</i>, <i>count</i> number of times. Basically it gives <b>term-frequency</b> of term 't' in document 'd' i.e. <i>tf(t, d).</i> Following image will illustrate.</p>\n",
    "\n",
    "<img src=\"./images/docword.jpg\"/>\n",
    "<p style=\"font-family: times new roman;font-size: 12px;\">Above image is for <b>docword.enron.txt</b>. Initially I was working on that bag of words but later changed</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: times new roman;font-size: 18px; font-weight: bold;\"> vocab.kos.txt </p>\n",
    "<p style=\"font-family: times new roman;font-size: 18px;\"> This file has lexicographically sorted list of words in the entire collection. These are <b>free from stop-words</b>. The <b>row number is the ID of the word</b>. Following image will illustrate</p>\n",
    "\n",
    "<img src=\"./images/vocab.jpg\"/>\n",
    "<p style=\"font-family: times new roman;font-size: 12px;\">Above image is for <b>vocab.enron.txt</b>. Initially I was working on that bag of words but later changed</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: times new roman;font-size: 18px;\"> Now that we have understood the data, let's dive in to converting this data into a form suitable for processing. We will also try to pre-process and visualize the data at times, but since the data is already processed and made ready to use, there won't be a need of it as such.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: times new roman; font-weight:bold;\">1. Data reading and conversion</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\shashwat kadam\\anaconda3\\lib\\site-packages (4.31.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shashwat kadam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install tqdm\n",
    "# Uncomment above line to install 'tqdm' progress bar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.auto import trange\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data') # data files are present in directory named 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docword_path = os.path.join(DATA_DIR, 'docword.kos.txt')\n",
    "vocab_path = os.path.join(DATA_DIR, 'vocab.kos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(docword_path, 'r')\n",
    "file_content = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = file_content.split('\\n') # separate the line and make a list\n",
    "print(\"displaying first 15 lines of content\")\n",
    "print(raw_data[:15])\n",
    "\n",
    "# Extract the metadata\n",
    "D, W, N = map(int, raw_data[:3]) # Or map(np.int32, raw_data[:3])\n",
    "\n",
    "# rest of the data is table\n",
    "table_data = raw_data[3:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Making a 2D table:')\n",
    "table_data = [row.split() for row in tqdm(table_data)] # Making a 2D table (tqdm is progress-bar)\n",
    "\n",
    "# Create a pandas dataframe\n",
    "table = pd.DataFrame(table_data, columns=['docID', 'wordID', 'count'], dtype=np.float32)\n",
    "\n",
    "print(\"Datatypes: {}\".format(table.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the vocabulary now\n",
    "f = open(vocab_path, 'r')\n",
    "file_content = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = file_content.split('\\n')\n",
    "print('few words: {}'.format(words[:10]))\n",
    "if len(words[-1]) == 0:\n",
    "    del words[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a vocabulary dictionary\n",
    "vocabulary = dict(enumerate(words, 1))\n",
    "# Display few items from the 'vocabulary'\n",
    "\n",
    "items = list(vocabulary.items())\n",
    "print('(wordID, word)')\n",
    "print(items[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: times new roman; font-weight:bold;\">2. Making <i>document vectors</i> and finding <i>Cosine Similarity</i></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: times new roman; font-weight:bold;\">2.1. Getting <i>tf-idf</i> scores</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document frequencied first\n",
    "document_freqs = table.groupby('wordID')['docID'].count()\n",
    "document_freqs = pd.DataFrame(document_freqs)\n",
    "document_freqs.rename(columns={'docID': 'df'}, inplace=True) # now the 'docID' column has accumulated the 'df's\n",
    "document_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IDF using the formula idf(t) = ln(D/df(t))\n",
    "dfs = np.array(document_freqs['df'])\n",
    "if 0 in dfs:\n",
    "    print(True) # To make sure there aren't any words whose df=0\n",
    "idfs = np.log(D/dfs)\n",
    "document_freqs['idf'] = idfs\n",
    "document_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have idfs of every term, let's compute tf-idf\n",
    "def get_tf_idf_score(tID, dID, idx):\n",
    "#     start = time()\n",
    "    rows = table.loc[table['docID']==dID, ['wordID', 'count']]\n",
    "    if tID in rows['wordID'].values:\n",
    "        tf = rows.loc[rows['wordID']==tID, 'count']\n",
    "    else:\n",
    "#         end = time()\n",
    "#         print('Time taken: {} s'.format(end-start))\n",
    "        return 0.0 # directly return tf_idf = 0.0 because tf = 0.0\n",
    "    idf = document_freqs.loc[tID]['idf'] if tID in idx else 0.0\n",
    "    tf_idf = tf * idf\n",
    "#     end = time()\n",
    "#     print('Time taken: {} s'.format(end-start))\n",
    "    return float(tf_idf)\n",
    "idx = document_freqs.index\n",
    "print(get_tf_idf_score(61, 1, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: times new roman; font-weight:bold;\">2.2. Making normalized document vectors</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_word_IDs(dID):\n",
    "    # returns list of wordID present in document having ID 'dID'\n",
    "    return np.array(table.loc[table['docID'] == dID, 'wordID'], dtype='int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vector(dID):\n",
    "    # Returns a numpy array which is the normalized document vector of document 'dID'.\n",
    "#     start = time()\n",
    "    trunc_doc_vector = get_valid_word_IDs(dID)\n",
    "    tf_idfs = [get_tf_idf_score(tID, dID, idx) for tID in trunc_doc_vector]\n",
    "    d1 = np.zeros((W+1,))\n",
    "    d1[trunc_doc_vector] = 1.0\n",
    "    d1[d1 == 1.0] = tf_idfs\n",
    "    # Normalize the vector\n",
    "    norm = np.linalg.norm(d1)\n",
    "    if norm == 0:\n",
    "        return d1\n",
    "    \n",
    "#     end = time()\n",
    "#     print('Time elapsed: {} s'.format(end-start))\n",
    "    return d1 / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_vecs = []\n",
    "for dID in tqdm(range(1, D+1), desc=\"Progress\", ncols=\"980\"):\n",
    "    doc_vecs.append(get_document_vector(dID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-family: times new roman; font-weight:bold;\">2.2.1. Saving the vectors on a file to avoid re-computation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.join(os.getcwd(), \"precomputed_data\")\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    print(\"Creating directory {}...\".format(OUTPUT_PATH))\n",
    "    os.mkdir(OUTPUT_PATH)\n",
    "    print(\"Directory {} created!\".format(OUTPUT_PATH))\n",
    "    \n",
    "FILE_NAME = \"kos_document_vectors.csv\"\n",
    "\n",
    "data_block = np.array(doc_vecs)\n",
    "data_block = pd.DataFrame(data_block)\n",
    "data_block.to_csv(os.path.join(OUTPUT_PATH, FILE_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: times new roman; font-weight:bold;\">2.3. Calculating <i>Cosine similarites</i></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the document vectors are normalized, simple dot product will give the value of 'cos' of angle between two documents\n",
    "doc_vecs = pd.read_csv(os.path.join(OUTPUT_PATH, FILE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs1 = np.array(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs = doc_vecs1[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = doc_vecs.shape\n",
    "print('No. of documents: {} No. of terms: {}'.format(rows, cols))\n",
    "\n",
    "similarity_matrix = np.zeros(shape=(rows, rows))\n",
    "print(similarity_matrix.shape)\n",
    "for i in trange(rows, ncols='980'):\n",
    "    for j in range(rows):\n",
    "        similarity_matrix[i, j] = np.dot(doc_vecs[i], doc_vecs[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rounding off to 7 decimal places\")\n",
    "similarity_matrix = np.around(similarity_matrix, decimals=7)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save similarity matrix to a CSV file to avoid recomputation\n",
    "file_data = pd.DataFrame(similarity_matrix)\n",
    "if os.path.exists(OUTPUT_PATH):\n",
    "    print(\"directory exists. Saving the file...\")\n",
    "    file_data.to_csv(os.path.join(OUTPUT_PATH, \"kos_similarity_matrix.csv\"))\n",
    "else:\n",
    "    print(\"Creating directory {}. And saving the file...\".format(OUTPUT_PATH))\n",
    "    os.mkdir(OUTPUT_PATH)\n",
    "    file_data.to_csv(os.path.join(OUTPUT_PATH, \"kos_similarity_matrix.csv\"))\n",
    "print(\"File saved. Check the directory {} to verify...\".format(OUTPUT_PATH))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_matrix = 1. - similarity_matrix\n",
    "print(\"Difference matrix:\")\n",
    "print(difference_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have both 'difference' and 'similarity' matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: times new roman; font-weight:bold;\">3. Clustering</h1>\n",
    "<p style=\"font-family: times new roman; font-size:18px;\"> Now we have calculated and saved document vectors and similarity matrix. If you want to recompute the vectors and matrix, run the code from <i>Section 2.</i> Otherwise just load the data from saved files. It will be much easier. If you want to save vectors and similarity matrix from other dataset, simply change the file paths accordingly, and re-run from <i>Section 2.</i> onwards</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: times new roman; font-weight:bold;\">3.1. Agglomerative Clustering (Hierarchical clustering)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "model = AgglomerativeClustering(affinity='precomputed', n_clusters=5, linkage='complete')\n",
    "model.fit(difference_matrix)\n",
    "# print(list(model.labels_))\n",
    "print(len(model.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(100,110))\n",
    "plt.title(\"Dendrogram\")\n",
    "plot_dendrogram(model, labels=model.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_from_labels(labels, n_clusters=5):\n",
    "    cluster_bag = {i+1:[] for i in range(n_clusters)}\n",
    "    for i in range(len(labels)):\n",
    "        cluster_bag[labels[i]+1].append(i+1)\n",
    "    return cluster_bag    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = get_clusters_from_labels(model.labels_, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in cb.items():\n",
    "    print(\"Cluster {} contains documents: {}]\\nTotal {} documents\".format(k, v, len(v)))\n",
    "    print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: times new roman; font-weight:bold;\">3.2. K-Means Clustering</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(init='k-means++', n_clusters=8, n_init=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs = pd.read_csv(\"./precomputed_data/kos_document_vectors.csv\")\n",
    "dvs = np.array(doc_vecs)\n",
    "dvs = dvs[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means.fit(dvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_bags = get_clusters_from_labels(k_means.labels_, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in cluster_bags.items():\n",
    "    print(\"Cluster {} contains documents: {}]\\nTotal {} documents\".format(k, v, len(v)))\n",
    "    print('*'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: times new roman; font-weight:bold;\">3.3. DBSCAN Clustering</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='auto', eps=1.0, leaf_size=30, metric='euclidean',\n",
       "    metric_params=None, min_samples=2, n_jobs=None, p=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "sim_mat = pd.read_csv(\"./precomputed_data/kos_similarity_matrix.csv\")\n",
    "sim_mat = np.array(sim_mat)\n",
    "sim_mat = sim_mat[:, 1:]\n",
    "diff_mat = 1 - sim_mat\n",
    "dbscan = DBSCAN(eps=1.0, min_samples=2)\n",
    "dbscan.fit(diff_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
